# AI Benchmark Repository

A specialized testing suite for evaluating Large Language Models (LLMs) on spatial reasoning tasks through gradient scoring systems.

## ğŸš€ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Run benchmark on LLM output file
python run_benchmark.py --input path_to_llm_output.txt

# Run and save to leaderboard
python run_benchmark.py --input llm_output.txt --add-to-leaderboard

# View the leaderboard
python run_benchmark.py --leaderboard
```

## ğŸ† Leaderboard System

The benchmark includes a leaderboard to track and compare model performance:

- **Add results**: Use `--add-to-leaderboard` when running benchmarks
- **View rankings**: Use `--leaderboard` to display current standings
- **Persistent storage**: Results saved to `leaderboard.json`

## ğŸ§ª Current Benchmarks

### The "Maze Gauntlet" - LLM Spatial Reasoning Challenge

Unlike traditional maze benchmarks that use binary pass/fail scoring, the Maze Gauntlet implements a **Gradient Scoring** system that evaluates how well LLMs can generate complex, solvable mazes with specific state-dependency rules.

#### Philosophy
Most maze benchmarks are binary (Pass/Fail). This is a **Gradient Benchmark** that rewards:
- **Ambition**: Grid size and complexity
- **Logic**: Proper S â†’ K â†’ D â†’ E path progression  
- **Danger**: Strategic trap placement adjacent to valid paths

## ğŸ“Š Scoring System

### The Maze Gauntlet Scoring Components:

1. **Ambition** (Grid Size)
   - Points: 100 Ã— logâ‚‚(Rows Ã— Cols)
   - Rewards larger mazes but with logarithmic scaling to prevent exponential runaway

2. **Progress** (Path Logic)
   - 2 points per reachable tile
   - +50 bonus for Key ('K') 
   - +50 bonus for Door ('D')
   - +50 bonus for End ('E')

3. **Path Efficiency** (New)
   - Points: (Shortest Valid Path Length / Grid Size) Ã— 100
   - Rewards mazes that use the available space efficiently for the solution

4. **Danger** (Strategic Placement)
   - Points: 20 Ã— sqrt(Adjacent Traps)
   - Diminishing returns preventing "trap spamming"
   - Only traps near the valid solution path count

5. **Logic Penalties**
   - If Traps > Walls: -50% score penalty
   - Path must follow S â†’ K â†’ D â†’ E sequence

6. **Proximity Bonuses**
   - Partial credit for **unreachable** objectives based on distance to reachable areas
   - No double-counting for reached objectives

7. **Constraints**
   - Maximum maze size: **64Ã—64** (mazes exceeding this limit score 0)

## ğŸ”§ Architecture

```
ai-benchmark/
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ CHANGELOG.md            # Version history
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ run_benchmark.py        # CLI Entry point
â”œâ”€â”€ leaderboard.py          # Leaderboard management
â”œâ”€â”€ leaderboard.json        # Stored benchmark results
â””â”€â”€ benchmarks/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ maze/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ prompt.md       # The "Anti-Cheese" Prompt
        â””â”€â”€ evaluator.py    # Gradient Scoring Logic
```

## ğŸ¯ Adding New Benchmarks

This repository is designed to be modular. To add new benchmarks:

1. Create a new directory under `benchmarks/`
2. Implement an evaluator function
3. Add a `prompt.md` file with the benchmark prompt
4. Update the CLI in `run_benchmark.py` to include your new benchmark

## ğŸ“ Example Usage

```bash
# Run the Maze Gauntlet benchmark on a file
python run_benchmark.py --input sample_llm_output.txt

# Add benchmark results to leaderboard
python run_benchmark.py --input sample_llm_output.txt --add-to-leaderboard

# Get JSON output
python run_benchmark.py --input sample_llm_output.txt --json
```

The output will be a detailed JSON report showing your score breakdown.