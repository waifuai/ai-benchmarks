# AI Benchmark Repository

A specialized testing suite for evaluating Large Language Models (LLMs) on spatial reasoning tasks through gradient scoring systems.

## ğŸ§ª Current Benchmarks

### The "Maze Gauntlet" - LLM Spatial Reasoning Challenge

Unlike traditional maze benchmarks that use binary pass/fail scoring, the Maze Gauntlet implements a **Gradient Scoring** system that evaluates how well LLMs can generate complex, solvable mazes with specific state-dependency rules.

#### Philosophy
Most maze benchmarks are binary (Pass/Fail). This is a **Gradient Benchmark** that rewards:
- **Ambition**: Grid size and complexity
- **Logic**: Proper S â†’ K â†’ D â†’ E path progression  
- **Danger**: Strategic trap placement adjacent to valid paths

## ğŸ† Leaderboard

| Rank | Model | Score | Date | Notes |
|------|-------|-------|------|-------|
| 1 | - | - | - | Waiting for submissions |
| 2 | - | - | - | |
| 3 | - | - | - | |

*Submit your results by running the benchmark and sharing your score!*

## ğŸš€ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Run benchmark on LLM output
python run_benchmark.py --input path_to_llm_output.txt
```

## ğŸ“Š Scoring System

### The Maze Gauntlet Scoring Components:

1. **Ambition** (Grid Size)
   - Points: Rows Ã— Columns
   - Rewards larger, more ambitious mazes

2. **Progress** (Path Logic)
   - 2 points per reachable tile
   - +50 bonus for Key ('K') 
   - +50 bonus for Door ('D')
   - +50 bonus for End ('E')

3. **Danger** (Strategic Placement)
   - +20 points per trap adjacent to valid path
   - Only traps near the solution path count
   - Traps in sealed rooms = 0 points

4. **Logic Penalties**
   - If Traps > Walls: -50% score penalty
   - Path must follow S â†’ K â†’ D â†’ E sequence

5. **Proximity Bonuses**
   - Partial credit for unreachable objectives based on distance to reachable areas

## ğŸ”§ Architecture

```
ai-benchmark/
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ run_benchmark.py        # CLI Entry point
â””â”€â”€ benchmarks/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ maze/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ prompt.md       # The "Anti-Cheese" Prompt
        â””â”€â”€ evaluator.py    # Gradient Scoring Logic
```

## ğŸ¯ Adding New Benchmarks

This repository is designed to be modular. To add new benchmarks:

1. Create a new directory under `benchmarks/`
2. Implement an evaluator function
3. Update the CLI in `run_benchmark.py` to include your new benchmark

## ğŸ“ Example Usage

```bash
# Run the Maze Gauntlet benchmark
python run_benchmark.py --input sample_llm_output.txt
```

The output will be a detailed JSON report showing your score breakdown.